<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
	<title>Etymo AI newsletter #13</title>
	<style type="text/css">
		#outlook a {padding:0;}
		body{width:100% !important; -webkit-text-size-adjust:100%; -ms-text-size-adjust:100%; margin:0; padding:0; font-family: "Open Sans", sans-serif;}
		.ExternalClass {width:100%;}
		.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div {line-height: 100%;}
		#backgroundTable {margin:0; padding:0; width:100% !important; line-height: 100% !important;}
		img {outline:none; text-decoration:none; -ms-interpolation-mode: bicubic;}
		a img {border:none;}
		.image_fix {display:block;}
		p {margin: 1em 0;}
		h1, h2, h4, h5, h6 {color: black !important; font-weight:300; text-align:center;}
		h3 {font-weight:300; text-align:center;}
		h1 a, h2 a, h3 a, h4 a, h5 a, h6 a {color: blue !important;}
		h1 a:active, h2 a:active,  h3 a:active, h4 a:active, h5 a:active, h6 a:active {
			color: red !important;
		}
		h2 {font-size:4rem;}
		h3 {font-size:1.75rem; color:blue;}
		h4 {font-size:1.3rem;}

		h1 a:visited, h2 a:visited,  h3 a:visited, h4 a:visited, h5 a:visited, h6 a:visited {
			color: purple !important;
		}
		table td {border-collapse: collapse;}
		table { border-collapse:collapse; mso-table-lspace:0pt; mso-table-rspace:0pt; }
		a {color: blue;}
		@media only screen and (max-device-width: 480px) {
			a[href^="tel"], a[href^="sms"] {
						text-decoration: none;
						color: black; /* or whatever your want */
						pointer-events: none;
						cursor: default;
					}

			.mobile_link a[href^="tel"], .mobile_link a[href^="sms"] {
						text-decoration: default;
						color: blue !important; /* or whatever your want */
						pointer-events: auto;
						cursor: default;
					}
		}

		@media only screen and (min-device-width: 768px) and (max-device-width: 1024px) {
			a[href^="tel"], a[href^="sms"] {
						text-decoration: none;
						color: blue;
						pointer-events: none;
						cursor: default;
					}

			.mobile_link a[href^="tel"], .mobile_link a[href^="sms"] {
						text-decoration: default;
						color: blue !important;
						pointer-events: auto;
						cursor: default;
					}
		}

		.section{
			padding:10px;
		}
		#backgroundTable{
			width:100%;
			max-width:800px;
			margin:0 auto;
		}
		th{
			text-align:left;
		}
		.padded td, th{
			padding:5px 10px;
		}

		li{
			margin:10px;
		}

		@media only screen and (-webkit-min-device-pixel-ratio: 2) {
			/* Put your iPhone 4g styles in here */
		}
		@media only screen and (-webkit-device-pixel-ratio:.75){
			/* Put CSS for low density (ldpi) Android layouts in here */
		}
		@media only screen and (-webkit-device-pixel-ratio:1){
			/* Put CSS for medium density (mdpi) Android layouts in here */
		}
		@media only screen and (-webkit-device-pixel-ratio:1.5){
			/* Put CSS for high density (hdpi) Android layouts in here */
		}
	</style>



</head>
<body>
	<table cellpadding="0" cellspacing="0" border="0" id="backgroundTable" style="background-color:#ffffff;">
	<tr>
		<td class="section" style="text-align:center;padding:10px;">
			<h1>
				<img class="image_fix" src="https://raw.githubusercontent.com/EtymoIO/newsletter/master/image/header.png" alt="newsletter.etymo" title="newsletter.etymo" style="width:100%	;max-width:600px; margin:0px auto;" />
			</h1>
		</td>
	</tr>
	<tr>
		<td class="section" style="text-align:center;">
			<h2 style="font-weight:300; font-size:2rem;">25th January - 7th February 2019</h2>
			<h3 style="color:blue;">
				1779 new papers
			</h3>
		</td>
	</tr>
	<tr>
		<td class="section">
			<p>
				In this newsletter from Etymo, you can find out the latest development in machine learning research, including the most popular datasets used, the most frequently appearing keywords, the important research papers associated with the keywords, and the most trending papers in the past two weeks.<br/><br/>
If you and your friends like this newsletter, you can subscribe to our fortnightly newsletters <a href="https://docs.google.com/forms/d/e/1FAIpQLScPzDcOp6gnVWtiXtLOG_uFff0Fg7uEuXqg0cu5LiCNkq637Q/viewform">here</a>.
			</p>
		</td>
	</tr>
	<tr>
		<td class="section">
			<h3>
				Fortnight Summary
			</h3>
		<p>
			There are 1779 papers published in the past two weeks. Computer vision (CV) is still a main research area, as reflected on the popularity of the CV datasets and the most trending papers.<br></br>

			We present the emerging interests in research under the "Trending Phrases" section. The papers in this section show some cutting edge results. There are four good papers, each of which is related to <b>Hyberbox based Machine Learning</b>, <b>Cycle GAN</b>, <b>Exact Baysian Inference</b> and <b>Reduced Basis</b>.<br></br>

			Other notable development in research includes the following:
			<li>
				More efficient models using lightweight and dynamic convolutions than self-attention models: <a href="https://arxiv.org/abs/1901.10430">Pay Less Attention with Lightweight and Dynamic Convolutions</a>
			</li>
			<li>
				Deep generative models for classification in ultra-sparse categorisation in training data: <a href="https://arxiv.org/abs/1901.08560">Semi-Unsupervised Learning with Deep Generative Models: Clustering and Classifying using Ultra-Sparse Labels</a>
			</li>
			<li>
				A high-level review of general liguistic intelligence and practices of existing language models: <a href="https://arxiv.org/abs/1901.11373">Learning and Evaluating General Linguistic Intelligence</a>
			</li>
			<li>
				A new generation of learning algorithms for recurrent neural networks: <a href="https://arxiv.org/abs/1901.09049">Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets</a>
			</li>
			<li>
				A study on the repeated selection (RS) choice representation: <a href="https://arxiv.org/abs/1809.05139">Choosing to Rank</a>
			</li>
			<li>
				A kernal approach to model selection for simulator-based statistical models: <a href="https://arxiv.org/abs/1902.02517">Model Selection for Simulator-based Statistical Models: A Kernel Approach</a>
			</li>
			<li>
				A New Ensemble Method for Heterogeneous Transfer Learning (including codes and datasets): <a href="https://arxiv.org/abs/1901.11459">Funnelling: A New Ensemble Method for Heterogeneous Transfer Learning and its Application to Polylingual Text Classification</a>
			</li>
			<li>
				The use of supervised machine learning (ML) as a tool to get more accurate comsmic shear measurement: <a href="https://arxiv.org/abs/1807.02120">Weak-lensing shear measurement with machine learning: teaching artificial neural networks about feature noise</a>
			</li>

			<br />
			Some of the notable review papers include:
			<li>
				<a href="https://arxiv.org/abs/1803.08823">A high-bias, low-variance introduction to Machine Learning for physicists</a>
			</li>
			<li>
				<a href="https://arxiv.org/abs/1803.04311">Deep Learning in Mobile and Wireless Networking: A Survey</a>
			</li>
			<li>
				<a href="https://arxiv.org/abs/1811.04364">A survey of state-of-the-art mixed data clustering algorithms</a>
			</li>
		</p>

		</td>
	</tr>
	<tr>
		<td class="section">
			<h3>
				Popular Datasets
			</h3>
			<p>
				Computer vision is still the main focus area of research.
			</p>
			<table class="padded">
			<tr>
			        <th>Name</th>
			        <th>Type</th>
			        <th>Number of Papers</th>
			</tr>
			<tr>
				<td><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a></td>
				<td>Handwritten Digits</td>
			        <td>96</td>
			</tr>
			<tr>
				<td><a href="http://www.image-net.org">ImageNet</a></td>
				<td>Image Dataset</td>
				<td>52</td>
			</tr>
			<tr>
				<td><a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a></td>
				<td>Tiny Image Dataset in 10 Classes</td>
				<td>48</td>
			</tr>
			<tr>
				<td><a href="http://cocodataset.org/#home">COCO</a></td>
				<td>Common Objects in Context</td>
				<td>14</td>
			</tr>
			<tr>
				<td><a href="http://ufldl.stanford.edu/housenumbers/">SVHN</a></td>
				<td>The Street View House Numbers Dataset</td>
				<td>12</td>
			</tr>
			<tr>
				<td><a href="http://www.cvlibs.net/datasets/kitti/">KITTI</a></td>
				<td>Autonomous Driving</td>
				<td>11</td>
			</tr>
			<tr>
				<td><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a></td>
				<td>Large-scale CelebFaces Attributes</td>
				<td>10</td>
			</tr>
			<tr>
				<td><a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-100</a></td>
				<td>Tiny Image Dataset in 100 Classes</td>
				<td>10</td>
			</tr>
			</table>
		</td>
	</tr>

	<tr>
		<td class="section">
			<h3>
				Trending Phrases
			</h3>
			<p>
				In this section, we present a list of phrases that appeared significantly more in this newsletter than the previous newsletters.
			</p>
			<ul> <b>Hyberbox based Machine Learning</b>:
				<li>
					<a href="https://arxiv.org/abs/1901.11303">Hyperbox based machine learning algorithms: A comprehensive survey</a>
				</li>
				<br /> <b>Cycle GAN</b>:
				<li>
					<a href="https://arxiv.org/abs/1902.00536">Comparison of Patch-Based Conditional Generative Adversarial Neural Net Models with Emphasis on Model Robustness for Use in Head and Neck Cases for MR-Only planning</a>
				</li>
				<br /> <b>Exact Baysian Inference</b>:
				<li>
					<a href="https://arxiv.org/abs/1901.09881">Scalable Metropolis-Hastings for Exact Bayesian Inference with Large Datasets</a>
				</li>
				<br /> <b>Reduced Basis</b>:
				<li>
					<a href="https://arxiv.org/abs/1902.00148">Bifidelity data-assisted neural networks in nonintrusive reduced-order modeling</a>
				</li>
			</ul>
		</td>
	</tr>


	<tr>
		<td>
			<h3>
				Etymo Trending
			</h3>
			<p>
				Presented below is a list of the most trending papers added in the last two weeks.
			</p>
			<ul>
				<li>
					<a href="https://arxiv.org/abs/1901.10430">Pay Less Attention with Lightweight and Dynamic Convolutions</a>:
					<br />
					The authors show that a very lightweight convolution can perform competitively to the best reported self-attention results. In the paper, they introduce dynamic convolutions which are simpler and more efficient than self-attention.
				</li>
				<br />
				<li>
					<a href="https://arxiv.org/abs/1901.08560">Semi-Unsupervised Learning with Deep Generative Models: Clustering and Classifying using Ultra-Sparse Labels</a>:
					<br />
					Semi-unsupervised learning is an extreme case of semi-supervised learning with ultra-sparse categorisation where some classes are sparsely labelled and other classes appear only as unlabelled data in the training data. The paper introduces two deep generative models for classification in this regime that extend previous deep generative models designed for semi-supervised learning.
				</li>
				<br />
				<li>
					<a href="https://arxiv.org/abs/1901.11373">Learning and Evaluating General Linguistic Intelligence</a>:
					<br />
					The authors define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. The analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation. In conclusion, many models require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. In addition, models are mostly overfitting to the quirks of particular datasets.
				</li>
			</ul>
		</td>
	</tr>

	<tr>
		<td class="section">
			<h3>
				Frequent Words
			</h3>

			<p>
				"Learning", "Model", "Data" and "Training" are the most frequent words. The top two papers associated with each of the key words are:
			</p>
			<ul> <b>Learning</b>:
				<li>
					<a href="https://arxiv.org/abs/1803.08823">A high-bias, low-variance introduction to Machine Learning for physicists</a>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1901.09049">Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets</a>
				</li>
				<br /> <b>Model</b>:
				<li>
					<a href="https://arxiv.org/abs/1809.05139">Choosing to Rank</a>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1902.02517">Model Selection for Simulator-based Statistical Models: A Kernel Approach</a>
				</li> 
				<br /> <b>Data</b>:
				<li>
					<a href="https://arxiv.org/abs/1803.04311">Deep Learning in Mobile and Wireless Networking: A Survey</a>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1811.04364">A survey of state-of-the-art mixed data clustering algorithms</a>
				</li>
				<br /> <b>Training</b>:
				<li>
					<a href="https://arxiv.org/abs/1901.11459">Funnelling: A New Ensemble Method for Heterogeneous Transfer Learning and its Application to Polylingual Text Classification</a>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1807.02120">Weak-lensing shear measurement with machine learning: teaching artificial neural networks about feature noise</a>
				</li>
			</ul>
		</td>
	</tr>

	<tr>
	<td class="section">
		<h4 style="line-height:1.25">
				Hope you have enjoyed this newsletter! If you have any comments or suggestions, please email ernest@etymo.io or steven@etymo.io.	
		</h4>
	</td>
	</tr>
	</table>

</body>
</html>
