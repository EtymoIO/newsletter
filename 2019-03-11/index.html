<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
	<title>Etymo AI newsletter #15</title>
	<style type="text/css">
		#outlook a {padding:0;}
		body{width:100% !important; -webkit-text-size-adjust:100%; -ms-text-size-adjust:100%; margin:0; padding:0; font-family: "Open Sans", sans-serif;}
		.ExternalClass {width:100%;}
		.ExternalClass, .ExternalClass p, .ExternalClass span, .ExternalClass font, .ExternalClass td, .ExternalClass div {line-height: 100%;}
		#backgroundTable {margin:0; padding:0; width:100% !important; line-height: 100% !important;}
		img {outline:none; text-decoration:none; -ms-interpolation-mode: bicubic;}
		a img {border:none;}
		.image_fix {display:block;}
		p {margin: 1em 0;}
		h1, h2, h4, h5, h6 {color: black !important; font-weight:300; text-align:center;}
		h3 {font-weight:300; text-align:center;}
		h1 a, h2 a, h3 a, h4 a, h5 a, h6 a {color: blue !important;}
		h1 a:active, h2 a:active,  h3 a:active, h4 a:active, h5 a:active, h6 a:active {
			color: red !important;
		}
		h2 {font-size:4rem;}
		h3 {font-size:1.75rem; color:blue;}
		h4 {font-size:1.3rem;}

		h1 a:visited, h2 a:visited,  h3 a:visited, h4 a:visited, h5 a:visited, h6 a:visited {
			color: purple !important;
		}
		table td {border-collapse: collapse;}
		table { border-collapse:collapse; mso-table-lspace:0pt; mso-table-rspace:0pt; }
		a {color: blue;}
		@media only screen and (max-device-width: 480px) {
			a[href^="tel"], a[href^="sms"] {
						text-decoration: none;
						color: black; /* or whatever your want */
						pointer-events: none;
						cursor: default;
					}

			.mobile_link a[href^="tel"], .mobile_link a[href^="sms"] {
						text-decoration: default;
						color: blue !important; /* or whatever your want */
						pointer-events: auto;
						cursor: default;
					}
		}

		@media only screen and (min-device-width: 768px) and (max-device-width: 1024px) {
			a[href^="tel"], a[href^="sms"] {
						text-decoration: none;
						color: blue;
						pointer-events: none;
						cursor: default;
					}

			.mobile_link a[href^="tel"], .mobile_link a[href^="sms"] {
						text-decoration: default;
						color: blue !important;
						pointer-events: auto;
						cursor: default;
					}
		}

		.section{
			padding:10px;
		}
		#backgroundTable{
			width:100%;
			max-width:800px;
			margin:0 auto;
		}
		th{
			text-align:left;
		}
		.padded td, th{
			padding:5px 10px;
		}

		li{
			margin:10px;
		}

		@media only screen and (-webkit-min-device-pixel-ratio: 2) {
			/* Put your iPhone 4g styles in here */
		}
		@media only screen and (-webkit-device-pixel-ratio:.75){
			/* Put CSS for low density (ldpi) Android layouts in here */
		}
		@media only screen and (-webkit-device-pixel-ratio:1){
			/* Put CSS for medium density (mdpi) Android layouts in here */
		}
		@media only screen and (-webkit-device-pixel-ratio:1.5){
			/* Put CSS for high density (hdpi) Android layouts in here */
		}
	</style>



</head>
<body>
	<table cellpadding="0" cellspacing="0" border="0" id="backgroundTable" style="background-color:#ffffff;">
	<tr>
		<td class="section" style="text-align:center;padding:10px;">
			<h1>
				<img class="image_fix" src="https://raw.githubusercontent.com/EtymoIO/newsletter/master/image/header.png" alt="newsletter.etymo" title="newsletter.etymo" style="width:100%	;max-width:600px; margin:0px auto;" />
			</h1>
		</td>
	</tr>
	<tr>
		<td class="section" style="text-align:center;">
			<h2 style="font-weight:300; font-size:2rem;">22nd February - 7th March 2019</h2>
			<h3 style="color:blue;">
				1941 new papers
			</h3>
		</td>
	</tr>
	<tr>
		<td class="section">
			<p>
				In this newsletter from Etymo, you can find out the latest development in machine learning research, including the most popular datasets used, the most frequently appearing keywords, the important research papers associated with the keywords, and the most trending papers in the past two weeks.<br/><br/>
If you and your friends like this newsletter, you can subscribe to our fortnightly newsletters <a href="https://docs.google.com/forms/d/e/1FAIpQLScPzDcOp6gnVWtiXtLOG_uFff0Fg7uEuXqg0cu5LiCNkq637Q/viewform">here</a>.
			</p>
		</td>
	</tr>
	<tr>
		<td class="section">
			<h3>
				Fortnight Summary
			</h3>
		<p>
			There are 1941 papers published in the past two weeks. Computer vision (CV) is still a main research area, as reflected on the popularity of the CV datasets and the most trending papers.<br></br>

			We present the emerging interests in research under the "Trending Phrases" section. The papers in this section show some cutting edge results. There are four good papers, each of which is related to <b>Process Mining</b>, <b>Time Parameter</b>, and <b>Causal Graph</b>.<br></br>

			Other notable development in research includes the following:
			<li>
				The topology of weight evolution in neural networks: <a href="https://arxiv.org/abs/1902.08160">Topology of Learning in Artificial Neural Networks</a>
			</li>
			<li>
				A conceptually simple and effective transfer learning approach without pretraining or finetuning: <a href="https://arxiv.org/abs/1902.10547">An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</a>
			</li>
			<li>
				A study on the quantum learnability of constant-depth classical circuits under the uniform distribution and in the distribution-independent framework of probably approximately correct learning: <a href="https://arxiv.org/abs/1903.02840">Quantum hardness of learning shallow classical circuits</a>
			</li>
			<li>
				A standard pruning technique to uncover subnetworks whose initializations made them capable of effective training: <a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a>
			</li>
			<li>
				To tackle the model update problem with the recurrent meta-learning framework: <a href="https://arxiv.org/abs/1806.07078">Learning to Update for Object Tracking with Recurrent Meta-learner</a>
			</li>
			<li>
				A new threat model by characterizing, developing and evaluating new attacks in the brokered learning setting, along with new defenses for these attacks: <a href="https://arxiv.org/abs/1811.09712">Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting</a>
			</li>
			<li>
				<a href="https://arxiv.org/abs/1903.01730">Probabilistic Modeling for Novelty Detection with Applications to Fraud Identification</a>
			</li>
			<li>
				<a href="https://arxiv.org/abs/1811.12359">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</a>
			</li>
			<li>
				A deep learning algorithm with Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods to efficiently compute uncertainty propagation for nonlinear PDEs: <a href="https://arxiv.org/abs/1903.03040">Deep learning observables in computational fluid dynamics</a>
			</li>
			<li>
				An investigation on the model inversion problem in the adversarial settings: <a href="https://arxiv.org/abs/1902.08552">Adversarial Neural Network Inversion via Auxiliary Knowledge Alignment</a>
			</li>

			<br />
			There is also one interesting paper discussing the most common myths in machine learning:
			<li>
				<a href="https://arxiv.org/abs/1902.06789">Seven Myths in Machine Learning Research</a>
			</li>
		</p>

		</td>
	</tr>
	<tr>
		<td class="section">
			<h3>
				Popular Datasets
			</h3>
			<p>
				Computer vision is still the main focus area of research.
			</p>
			<table class="padded">
			<tr>
			        <th>Name</th>
			        <th>Type</th>
			        <th>Number of Papers</th>
			</tr>
			<tr>
				<td><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a></td>
				<td>Handwritten Digits</td>
			        <td>72</td>
			</tr>
			<tr>
				<td><a href="http://www.image-net.org">ImageNet</a></td>
				<td>Image Dataset</td>
				<td>54</td>
			</tr>
			<tr>
				<td><a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a></td>
				<td>Tiny Image Dataset in 10 Classes</td>
				<td>41</td>
			</tr>
			<tr>
				<td><a href="http://cocodataset.org/#home">COCO</a></td>
				<td>Common Objects in Context</td>
				<td>27</td>
			</tr>
			<tr>
				<td><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a></td>
				<td>Large-scale CelebFaces Attributes</td>
				<td>19</td>
			</tr>
			<tr>
				<td><a href="http://www.cvlibs.net/datasets/kitti/">KITTI</a></td>
				<td>Autonomous Driving</td>
				<td>14</td>
			</tr>
			<tr>
				<td><a href="https://www.cityscapes-dataset.com/">Cityscapes</a></td>
				<td>Images from 50 different cities</td>
				<td>11</td>
			</tr>
			</table>
		</td>
	</tr>

	<tr>
		<td class="section">
			<h3>
				Trending Phrases
			</h3>
			<p>
				In this section, we present a list of phrases that appeared significantly more in this newsletter than the previous newsletters.
			</p>
			<ul> <b>Process Mining</b>:
				<li>
					<a href="https://arxiv.org/abs/1902.08740">Behavioral Petri Net Mining and Automated Analysis for Human-Computer Interaction Recommendations in Multi-Application Environments</a>
				</li>
				<br /> <b>Time Parameter</b>:
				<li>
					<a href="https://arxiv.org/abs/1902.10369">Counting to Ten with Two Fingers: Compressed Counting with Spiking Neurons</a>
				</li>
				<br /> <b>Causal Graph</b>:
				<li>
					<a href="https://arxiv.org/abs/1903.01672">Causal Discovery and Hidden Driving Force Estimation from Nonstationary/Heterogeneous Data</a>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1902.01073">Causal Effect Identification from Multiple Incomplete Data Sources: A General Search-based Approach</a>
				</li>
			</ul>
		</td>
	</tr>


	<tr>
		<td>
			<h3>
				Etymo Trending
			</h3>
			<p>
				Presented below is a list of the most trending papers added in the last two weeks.
			</p>
			<ul>
				<li>
					<a href="https://arxiv.org/abs/1902.08160">Topology of Learning in Artificial Neural Networks</a>:
					<br />
					The authors study the emergence of structure in the weights by applying methods from topological data analysis. They train simple feedforward neural networks on the MNIST dataset and monitor the evolution of the weights. When initialized to zero, the weights follow trajectories that branch off recurrently, thus generating trees that describe the growth of the effective capacity of each layer. When initialized to tiny random values, the weights evolve smoothly along two-dimensional surfaces. They show that natural coordinates on these learning surfaces correspond to important factors of variation.
				</li>
				<br />
				<li>
					<a href="https://arxiv.org/abs/1902.10547">An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</a>:
					<br />
					This paper presents a conceptually simple and effective transfer learning approach that combines the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. The method does not require pretraining or finetuning separate components of the network and models can be trained end-to-end in a single step. 
				</li>
				<br />
				<li>
					<a href="https://arxiv.org/abs/1902.06789">Seven Myths in Machine Learning Research</a>:
					<br />
					In this research paper, the authors present seven myths commonly believed to be true in machine learning research:
					<ol style="list-style-type: square; padding-bottom: 0;">
					<li>Myth 1: TensorFlow is a Tensor manipulation library </li>
					<li>Myth 2: Image datasets are representative of real images found in the wild </li>
					<li>Myth 3: Machine Learning researchers do not use the test set for validation </li>
					<li>Myth 4: Every datapoint is used in training a neural network </li>
					<li>Myth 5: We need (batch) normalization to train very deep residual networks </li>
					<li>Myth 6: Attention > Convolution </li>
					<li>Myth 7: Saliency maps are robust ways to interpret neural networks</li>
					</ol>
				</li>
			</ul>
		</td>
	</tr>

	<tr>
		<td class="section">
			<h3>
				Frequent Words
			</h3>

			<p>
				"Learning", "Model", "Data" and "Training" are the most frequent words. The top two papers associated with each of the key words are:
			</p>
			<ul> <b>Learning</b>:
				<li>
					<a href="https://arxiv.org/abs/1903.02840">Quantum hardness of learning shallow classical circuits</a>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a>
				</li>
				<br /> <b>Model</b>:
				<li>
					<a href="https://arxiv.org/abs/1806.07078">Learning to Update for Object Tracking with Recurrent Meta-learner</a>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1811.09712">Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting</a>
				</li> 
				<br /> <b>Data</b>:
				<li>
					<a href="https://arxiv.org/abs/1903.01730">Probabilistic Modeling for Novelty Detection with Applications to Fraud Identification</a>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1811.12359">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</a>
				</li>
				<br /> <b>Network</b>:
				<li>
					<a href="https://arxiv.org/abs/1903.03040">Deep learning observables in computational fluid dynamics</a>
				</li>
				<li>
					<a href="https://arxiv.org/abs/1902.08552">Adversarial Neural Network Inversion via Auxiliary Knowledge Alignment</a>
				</li>
			</ul>
		</td>
	</tr>

	<tr>
	<td class="section">
		<h4 style="line-height:1.25">
				Hope you have enjoyed this newsletter! If you have any comments or suggestions, please email ernest@etymo.io or steven@etymo.io.	
		</h4>
	</td>
	</tr>
	</table>

</body>
</html>
